{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Dichotomiser 3 (ID3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Entropy\n",
    "#### Input\n",
    "list of probabilities. \n",
    "#### Return\n",
    "The entropy =  $\\sum_{i=0}^{N-1} -P_i$ * $log$<sub>2</sub>$P_i$\n",
    "\n",
    "#### Try except statement\n",
    "To handle the case of $P=0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(p):\n",
    "    e = 0\n",
    "    for pi in p:\n",
    "        try:\n",
    "            e = e + -pi * math.log(pi, 2)\n",
    "        except ValueError:\n",
    "            e = e + 0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Global Entropy\n",
    "Calculates the golbal entropy for a given dataset.\n",
    "#### Input\n",
    "Pandas dataframe\n",
    "#### Return\n",
    "Float represents the global entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_entropy(dataset):\n",
    "    classes = get_class_column(dataset)\n",
    "    unique_classes = classes.unique()\n",
    "    p = []\n",
    "    for c in unique_classes:\n",
    "        p.append(calculate_probability(c, classes))\n",
    "    return calculate_entropy(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-Value-Information-Map\n",
    "Takes the class column and a feature column and creates a dictionary that maps the feature values {key_1} to another dictionary {value_1} that maps the classes {key_2} to number of records that belongs to this class{value_1}. This is a step to calculate the information gain. \n",
    "#### Input\n",
    "feature_values: pandas dataframe of one column\n",
    "class_values: pandas dataframe of one column\n",
    "#### Return\n",
    "feature_value_information_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_value_information_map(feature_values, class_values):\n",
    "    classes = class_values.unique()\n",
    "    unique_feature_values = feature_values.unique()\n",
    "    feature_info_map = dict()\n",
    "    for f in unique_feature_values:\n",
    "        feature_info_map[f] = dict()\n",
    "    for f in unique_feature_values:\n",
    "        for c in classes:\n",
    "            _myMap = feature_info_map[f]\n",
    "            _myMap[c] = 0\n",
    "    for f, c in zip(feature_values, class_values):\n",
    "        _myMap = feature_info_map[f]\n",
    "        _myMap[c] = _myMap[c] + 1\n",
    "    return feature_info_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Information Gain\n",
    "#### Input \n",
    "dataset: pandas dataframe\n",
    "feature_name: string of the feature name.\n",
    "#### Return\n",
    "Information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(dataset, feature_name):\n",
    "    feature_values = dataset[feature_name]\n",
    "    class_values = get_class_column(dataset)\n",
    "    unique_feature_values = feature_values.unique()\n",
    "    classes = class_values.unique()\n",
    "    f_v_info_map = feature_value_information_map(feature_values, class_values)\n",
    "    entropy = []\n",
    "    summ = []\n",
    "    for f in unique_feature_values:\n",
    "        summation = 0\n",
    "        p = []\n",
    "        for c in classes:\n",
    "            _myMap = f_v_info_map[f]\n",
    "            summation = summation + _myMap[c]\n",
    "        for c in classes:\n",
    "            _myMap = f_v_info_map[f]\n",
    "            val = _myMap[c]\n",
    "            p.append(val / summation)\n",
    "        summ.append(summation)\n",
    "        entropy.append(calculate_entropy(p))\n",
    "    weighted_sum = 0\n",
    "    for i in range(len(entropy)):\n",
    "        weighted_sum = weighted_sum + (entropy[i] * (summ[i] / len(feature_values)))\n",
    "    global_entropy = calculate_global_entropy(dataset)\n",
    "    information_gain = global_entropy - weighted_sum\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Root\n",
    "Chooses the feature that has the highest information gain.\n",
    "#### Input\n",
    "dataset : pandas dataframe\n",
    "#### Return\n",
    "String : name of the most informative feature.\n",
    "##### Note\n",
    "In case of two equal information gains (and this value is the maximum), The function returns the first feature in order in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_root(dataset):\n",
    "    features = get_features(dataset)\n",
    "    information_gain = []\n",
    "    for f in features:\n",
    "        information_gain.append(calculate_information_gain(dataset, f))\n",
    "    ind = information_gain.index(np.max(information_gain))\n",
    "    return np.asarray(features)[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value-Data-Map\n",
    "Splits the dataset into a map according to the different classes (values) of a certain feature.\n",
    "{key = feature value : value = dataframe of the dataset} \n",
    "##### Input \n",
    "dataset : pandas dataframe\n",
    "feature : string of feature name\n",
    "##### Return\n",
    "value_data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_data_map(dataset, feature):\n",
    "    class_column_list = dataset[feature]\n",
    "    classes = class_column_list.unique()\n",
    "    class_column_list = list(class_column_list)\n",
    "    class_data_map = dict()\n",
    "    for c in classes:\n",
    "        class_data_map[c] = []\n",
    "    for c in classes:\n",
    "        for i in range(len(class_column_list)):\n",
    "            if class_column_list[i] == c:\n",
    "                class_data_map[c].append(dataset.iloc[i])\n",
    "    for c in classes:\n",
    "        class_data_map[c] = pd.DataFrame(class_data_map[c])\n",
    "        class_data_map[c] = class_data_map[c].drop(feature, axis=1)\n",
    "    return class_data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can Decide\n",
    "Decide if it is possible to predict the class of a record (sample) or not.\n",
    "#### Input\n",
    "record : pandas dataframe\n",
    "val_data_map : the dictionary returned from the value_data_map function\n",
    "root : String of the most informative feature.\n",
    "#### Return\n",
    "True : if all class values belong to the same class.\n",
    "False : if it is possible to make more iterations toward less uncertainty.\n",
    "Prob : String that is a flag to choose a class of the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_decide(record, val_data_map, root):\n",
    "    c = np.asarray(record[root])[0]\n",
    "    data = val_data_map[c]\n",
    "    classes = get_class_column(data)\n",
    "    features = get_features(data)\n",
    "    if len(features) == 0:\n",
    "        return 'prob'\n",
    "    elif len(classes.unique()) > 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict \n",
    "#### Input\n",
    "dataset : pandas dataframe\n",
    "record : pandas dataframe of one sample\n",
    "#### Return\n",
    "return the class if certainty is one or float of probability of class equals one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, record):\n",
    "    root = choose_root(dataset)\n",
    "    v_d_map = value_data_map(dataset, root)\n",
    "    c = np.asarray(record[root])[0]\n",
    "    try:  # some error rise say that this class doesn't exist in the dictionary.\n",
    "        data = v_d_map[c]\n",
    "    except:\n",
    "        prob = calculate_probability(1, get_class_column(dataset))\n",
    "        return prob\n",
    "    if can_decide(record, v_d_map, root) == 'prob':\n",
    "        prob = calculate_probability(1, get_class_column(data))\n",
    "        return prob\n",
    "    if can_decide(record, v_d_map, root):\n",
    "        return list(get_class_column(data))[0]\n",
    "    else:\n",
    "        return predict(data, record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "#### Transform continuous attributes into discrete \n",
    "* Turn \"age\" attribute into 5 classes based on values.\n",
    "* Turn \"height\" attribute into 4 classes based on values.\n",
    "* Turn \"weight\" attribute into 4 classes based on values.\n",
    "#### \"ap_hi\" attribute\n",
    "* Remove negative values.\n",
    "* Edit unaccepted values that ranges form 1 to 25 by a proper multiplication\n",
    "* Edit unaccepted values that is ranges from 700 to 16000 by a proper devision.\n",
    "* Edit two values of 309 and 401 as they are unique.\n",
    "#### \"ap_lo\" attribute\n",
    "* Remove negative values.\n",
    "* Edit unaccepted values that is ranges from 500 to max by a proper devision.\n",
    "* Replace unaccepted (very low) values with the mean to reduce number of classes.\n",
    "* Reduce number of classes to iclude only multiplications of 10 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "Divide the training dataset into 5 groups. And apply the ID3 algorithm to each testing record with respect to each group of data and decide the class based on the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_1 = cardio_data[1000:3000]\n",
    "train_set_2 = cardio_data[3000:5000]\n",
    "train_set_3 = cardio_data[5000:7000]\n",
    "train_set_4 = cardio_data[7000:9000]\n",
    "train_set_5 = cardio_data[9000:11000]\n",
    "test_set = cardio_data[0:999]\n",
    "test_set_labels = np.asarray(test_set.pop('cardio'))\n",
    "result_labels = []\n",
    "start_time = time.time()\n",
    "for i in range(len(test_set)):\n",
    "    decision = 0\n",
    "    p1 = predict(train_set_1, test_set.iloc[[i]]))\n",
    "    if p1 < 0.5:\n",
    "        decision = decision - 1\n",
    "    else :\n",
    "        decision = decision + 1\n",
    "    p2 = predict(train_set_2, test_set.iloc[[i]]))\n",
    "    if p2 < 0.5:\n",
    "        decision = decision - 1\n",
    "    else :\n",
    "        decision = decision + 1\n",
    "    p3 = predict(train_set_3, test_set.iloc[[i]]))\n",
    "    if p3 < 0.5:\n",
    "        decision = decision - 1\n",
    "    else :\n",
    "        decision = decision + 1\n",
    "    p4 = predict(train_set_4, test_set.iloc[[i]]))\n",
    "    if p4 < 0.5:\n",
    "        decision = decision - 1\n",
    "    else :\n",
    "        decision = decision + 1\n",
    "    p5 = predict(train_set_5, test_set.iloc[[i]]))\n",
    "    if p5 < 0.5:\n",
    "        decision = decision - 1\n",
    "    else :\n",
    "        decision = decision + 1\n",
    "    if decision < 0:\n",
    "        result_labels.append(0)\n",
    "    else :\n",
    "        result_labels.append(1)\n",
    "accuracy = 0\n",
    "for i in range(len(result_labels)):\n",
    "    if result_labels[i] == test_set_labels[i]:\n",
    "        accuracy = accuracy + 1\n",
    "print(accuracy/len(result_labels))\n",
    "result_df = pd.DataFrame(zip(result_labels, test_set_labels),\n",
    "                         columns=['Result labels', 'Actual labels'])\n",
    "compression_opts = dict(method='zip', archive_name='result.csv')\n",
    "result_df.to_csv('results.zip', index=False, compression=compression_opts)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
